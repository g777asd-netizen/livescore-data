import requests
from bs4 import BeautifulSoup
import json
import random
import time
from datetime import datetime

def scrape_scoreaxis():
    url = "https://www.scoreaxis.com/"
    
    # Ù‚Ø§Ø¦Ù…Ø© Ø¨Ù…ØªØµÙØ­Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ù„Ù„Ø®Ø¯Ø§Ø¹
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:123.0) Gecko/20100101 Firefox/123.0'
    ]
    
    headers = {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Referer': 'https://www.google.com/',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

    try:
        # Ù†Ù†ØªØ¸Ø± Ø«Ø§Ù†ÙŠØ© Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø´Ùƒ
        time.sleep(2)
        
        session = requests.Session()
        response = session.get(url, headers=headers, timeout=15)
        
        if response.status_code == 403:
            print("âŒ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù„Ø§ ÙŠØ²Ø§Ù„ ÙŠØ­Ø¸Ø± Ø§Ù„Ø§ØªØµØ§Ù„ (Error 403).")
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ø§Ø¨Ø· Widget Ù…Ø¨Ø§Ø´Ø± (Ø£Ø³Ù‡Ù„ ÙÙŠ Ø§Ù„Ø³Ø­Ø¨)
            url = "https://www.scoreaxis.com/widget/live-matches/8920" 
            print("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø¹ Ø±Ø§Ø¨Ø· Ø§Ù„Ù€ Widget...")
            response = session.get(url, headers=headers, timeout=15)

        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        matches_data = []

        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù€ Widget (Ø¨Ù†ÙŠØ© Ù…Ø®ØªÙ„ÙØ© Ù‚Ù„ÙŠÙ„Ø§Ù‹ ÙˆØ£Ø³Ù‡Ù„)
        match_rows = soup.find_all('div', class_='match-row') # Ù…Ø­Ø§ÙˆÙ„Ø© 1
        
        if not match_rows:
            match_rows = soup.select('.match-container, .event-row') # Ù…Ø­Ø§ÙˆÙ„Ø© 2

        print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(match_rows)} Ø¹Ù†ØµØ± Ù…Ø­ØªÙ…Ù„.")

        for item in match_rows:
            try:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø§Ù„ÙˆØ¯Ø¬Øª
                home = item.find(class_='home').text.strip()
                away = item.find(class_='away').text.strip()
                score = item.find(class_='score').text.strip()
                
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØªÙŠØ¬Ø©
                if not score: score = "VS"
                
                matches_data.append({
                    "home": home,
                    "away": away,
                    "score": score,
                    "time": "LIVE"
                })
            except:
                continue

        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ù†Ø¶Ø¹ Ø±Ø³Ø§Ù„Ø© Ø­Ø§Ù„Ø©
        if not matches_data:
            print("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¨Ø§Ø±ÙŠØ§ØªØŒ Ù‚Ø¯ ØªÙƒÙˆÙ† Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª ØªØºÙŠØ±Øª.")
            matches_data.append({
                "home": "No Live Matches",
                "away": "Try Later",
                "score": "-",
                "time": datetime.now().strftime("%H:%M")
            })
        else:
            print(f"âœ… ØªÙ… Ø³Ø­Ø¨ {len(matches_data)} Ù…Ø¨Ø§Ø±Ø§Ø© Ø¨Ù†Ø¬Ø§Ø­.")

        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
        with open('matches.json', 'w', encoding='utf-8') as f:
            json.dump(matches_data, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"âŒ Error: {e}")
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ù…Ù„Ù Ù„Ù†Ø±Ø§Ù‡
        with open('matches.json', 'w', encoding='utf-8') as f:
            json.dump([{"home": "Error", "away": str(e), "score": "X"}], f)

if __name__ == "__main__":
    scrape_scoreaxis()
