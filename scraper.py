import requests
from bs4 import BeautifulSoup
import json
import re

def scrape_scoreaxis_direct():
    # Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ø¨Ø§Ø´Ø± Ù„Ù…ÙˆÙ‚Ø¹ Scoreaxis
    url = "https://www.scoreaxis.com/"
    
    print("ğŸš€ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ù…ÙˆÙ‚Ø¹ Scoreaxis...")

    # Ø±Ø¤ÙˆØ³ Ù…Ø®ØµØµØ© Ù„Ù„Ø®Ø¯Ø§Ø¹ (ÙƒØ£Ù†Ù†Ø§ Ù…ØªØµÙØ­ Ù„Ø§Ø¨ØªÙˆØ¨ Ø­Ù‚ÙŠÙ‚ÙŠ)
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Referer': 'https://www.google.com/',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }

    try:
        session = requests.Session()
        response = session.get(url, headers=headers, timeout=15)
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ø¥Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙØªØ­ ÙˆÙ…Ø´ Ø¹Ø§Ù…Ù„ Ø¨Ù„ÙˆÙƒ
        if response.status_code == 403:
            print("âŒ Ù„Ù„Ø£Ø³Ù: Ù…ÙˆÙ‚Ø¹ Scoreaxis Ø­Ø¸Ø± Ø§Ù„Ø§ØªØµØ§Ù„ (Error 403 Cloudflare).")
            # Ù‡Ù†Ø§ Ø¨Ù†Ø­Ø§ÙˆÙ„ Ù†Ø¬Ø±Ø¨ Ø±Ø§Ø¨Ø· ØªØ§Ù†ÙŠ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¬ÙˆÙ‡ Ø§Ù„Ù…ÙˆÙ‚Ø¹
            url = "https://www.scoreaxis.com/fixtures-results"
            print("ğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø¹ Ø±Ø§Ø¨Ø· Ø§Ù„Ù†ØªØ§Ø¦Ø¬...")
            response = session.get(url, headers=headers, timeout=15)

        if response.status_code != 200:
            print(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„ØŒ Ø§Ù„ÙƒÙˆØ¯: {response.status_code}")
            return

        print("âœ… ØªÙ… ÙØªØ­ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø¨Ù†Ø¬Ø§Ø­! Ø¬Ø§Ø±ÙŠ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª...")

        soup = BeautifulSoup(response.text, 'html.parser')
        all_matches = []

        # ØªØ­Ù„ÙŠÙ„ ÙƒÙˆØ¯ HTML Ø§Ù„Ø®Ø§Øµ Ø¨Ù€ Scoreaxis
        # Ø¨Ù†Ø¨Ø­Ø« Ø¹Ù† Ø­Ø§ÙˆÙŠØ§Øª Ø§Ù„Ù…Ø¨Ø§Ø±ÙŠØ§Øª (ØªØ®ØªÙ„Ù Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª Ø£Ø­ÙŠØ§Ù†Ø§Ù‹ Ù„Ø°Ø§ Ù†Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ø¹Ø§Ù…)
        
        # Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø¨Ø§Ø±ÙŠØ§Øª ØªÙƒÙˆÙ† Ø¯Ø§Ø®Ù„ Ø¬Ø¯ÙˆÙ„ Ø£Ùˆ Ø¯ÙŠÙØ§Øª Ø¨Ø£Ø³Ù…Ø§Ø¡ ÙØ±Ù‚
        match_containers = soup.find_all('div', class_=re.compile('match|fixture')) 
        
        # Ù„Ùˆ Ù…Ù„Ù‚Ø§Ø´ØŒ Ù†Ø¬Ø±Ø¨ Ù†Ø¯ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø³Ù…Ø§Ø¡ ÙØ±Ù‚
        if not match_containers:
            match_containers = soup.find_all('tr')

        count = 0
        for item in match_containers:
            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ù‡ÙŠÙƒÙ„ Scoreaxis
                # Ù‡Ø°Ù‡ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª ØªÙ‚Ø±ÙŠØ¨ÙŠØ© ÙˆØªØ­ØªØ§Ø¬ ØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø­ÙŠ
                home_elem = item.find(class_=re.compile('home|team-1'))
                away_elem = item.find(class_=re.compile('away|team-2'))
                score_elem = item.find(class_=re.compile('score|result'))
                status_elem = item.find(class_=re.compile('time|status'))

                if home_elem and away_elem:
                    home = home_elem.get_text(strip=True)
                    away = away_elem.get_text(strip=True)
                    
                    # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØªÙŠØ¬Ø©
                    score = score_elem.get_text(strip=True) if score_elem else "VS"
                    status = status_elem.get_text(strip=True) if status_elem else "-"

                    # ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ø¨Ø§Ø±ÙŠØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
                    if not home or not away:
                        continue

                    all_matches.append({
                        "league": "Scoreaxis", # Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù„Ø§ ÙŠØ³Ù‡Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ø§Ù„Ø¯ÙˆØ±ÙŠ Ø¨Ø³Ù‡ÙˆÙ„Ø© Ù…Ù† Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø®ØªØµØ±Ø©
                        "date": "Today",
                        "home": home,
                        "away": away,
                        "home_score": score.split('-')[0].strip() if '-' in score else score,
                        "away_score": score.split('-')[1].strip() if '-' in score else "",
                        "home_logo": "", # ØµÙˆØ± Ø§Ù„Ù„ÙˆØ¬Ùˆ ØªØ­ØªØ§Ø¬ ÙƒÙˆØ¯ Ù…Ø¹Ù‚Ø¯ Ù„Ø³Ø­Ø¨Ù‡Ø§ Ù…Ù† Ø§Ù„Ø®Ù„ÙÙŠØ©
                        "away_logo": "",
                        "status": status,
                        "live": "Live" in status or "'" in status
                    })
                    count += 1
            except:
                continue

        print(f"ğŸ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(all_matches)} Ù…Ø¨Ø§Ø±Ø§Ø© Ù…Ù† Scoreaxis.")
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
        with open('matches.json', 'w', encoding='utf-8') as f:
            json.dump(all_matches, f, ensure_ascii=False, indent=2)

    except Exception as e:
        print(f"âŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹: {e}")

if __name__ == "__main__":
    scrape_scoreaxis_direct()
